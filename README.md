# ğŸ›©ï¸ Jet Engine Predictive Maintenance â€” RUL Prediction

End-to-end **Remaining Useful Life (RUL)** prediction for jet engines using the NASA C-MAPSS FD001 dataset. This project demonstrates a complete machine learning pipeline with both traditional ML models and deep learning (LSTM).

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)
![scikit-learn](https://img.shields.io/badge/scikit--learn-1.x-green.svg)

---

## ğŸ“‹ Project Overview

**Goal:** Predict how many operational cycles a jet engine has left before failure.

**Strategy:** Instead of predicting RUL directly, we predict **total lifespan** (max cycles until failure), then derive RUL as:
```
RUL = predicted_lifespan - current_cycle
```

This approach avoids data leakage and provides more stable predictions.

---

## ğŸ—ï¸ Architecture

```
Raw Data (train/test/RUL .txt files)
    â†“
Per-Engine Feature Engineering (aggregations + trends)
    â†“
Model Training (ML + LSTM)
    â†“
Lifespan Prediction â†’ RUL Conversion
    â†“
Online RUL Curves + Dashboard Visualization
```

---

## ğŸ“Š Models Compared

| Model | RUL RMSE (Test) | Notes |
|-------|-----------------|-------|
| ğŸ¥‡ **RandomForest** | ~46 cycles | Best performer |
| ğŸ¥ˆ GradientBoosting | ~49 cycles | Strong alternative |
| ğŸ¥‰ LSTM | ~73 cycles | Deep learning baseline |
| Ridge | ~82 cycles | Regularized linear |
| LinearRegression | ~186 cycles | Simple baseline |
| Lasso | ~185 cycles | Sparse features |

---

## ğŸ”§ Features

### Feature Engineering
- **Aggregated features:** mean, std, min, max of all sensors
- **Trend features:** slope, range, delta from mean
- **137 total features** per engine

### Models
- **Traditional ML:** LinearRegression, Lasso, Ridge, RandomForest, GradientBoosting
- **Deep Learning:** LSTM neural network (sequence-based)

### Visualizations
- 4-panel dashboard (model comparison, RUL curves, distributions, scatter)
- Multi-engine RUL prediction curves
- Pretty console output with rankings

---

## ğŸš€ Quick Start

### 1. Clone the repository
```bash
git clone https://github.com/YOUR_USERNAME/RUL-for-Nasa-Jet-Engines.git
cd RUL-for-Nasa-Jet-Engines
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

### 3. Run the pipeline
```bash
python main.py
```

### 4. Check outputs
- Console: Model rankings and metrics
- `outputs/`: Saved visualization plots

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ main.py                 # Main pipeline script
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ outputs/               # Generated plots (auto-created)
â”‚   â”œâ”€â”€ rul_dashboard.png
â”‚   â””â”€â”€ rul_multiple_engines.png
â””â”€â”€ local_assets/          # Data files
    â”œâ”€â”€ train_FD001.txt
    â”œâ”€â”€ test_FD001.txt
    â””â”€â”€ RUL_FD001.txt
```

---

## ğŸ“ˆ Sample Output

### Model Comparison
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              ğŸ“Š MODEL RANKINGS (by RUL RMSE)               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ¥‡ RandomForest       RUL RMSE:   45.97                   â•‘
â•‘ ğŸ¥ˆ GradientBoosting   RUL RMSE:   49.05                   â•‘
â•‘ ğŸ¥‰ LSTM               RUL RMSE:   72.69                   â•‘
â•‘ 4. Ridge              RUL RMSE:   81.56                   â•‘
â•‘ 5. Lasso              RUL RMSE:  185.23                   â•‘
â•‘ 6. LinearRegression   RUL RMSE:  185.88                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ“š Dataset

**NASA C-MAPSS FD001:**
- 100 training engines (run to failure)
- 100 test engines (truncated before failure)
- Single operating condition (sea level)
- Single fault mode (HPC degradation)
- 26 columns: unit_number, time_in_cycles, 3 operational settings, 21 sensors

---

## ğŸ§  Key Learnings

1. **Traditional ML can outperform Deep Learning** on small datasets (100 engines)
2. **Feature engineering matters** â€” aggregated + trend features capture degradation patterns
3. **Lifespan prediction** is more stable than direct RUL prediction
4. **LSTM requires more data** to show its full potential

---

## ğŸ› ï¸ Technologies

- **Python 3.8+**
- **pandas, numpy** â€” Data manipulation
- **scikit-learn** â€” ML models and preprocessing
- **TensorFlow/Keras** â€” LSTM neural network
- **matplotlib** â€” Visualization

---

## ğŸ“„ License

MIT License â€” Feel free to use and modify.

---

## ğŸ‘¤ Author

Built as a Data Science portfolio project demonstrating:
- End-to-end ML pipeline development
- Feature engineering for time-series data
- Model comparison and evaluation
- Deep learning integration
- Professional code structure and visualization
